{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RA challenge - training and prediction\n",
    "This notebook was used to train all the models used in the RA challenge,\n",
    "to test performance, and to generate the Docker directory used to build the submitted image.\n",
    "This should be run in a Jupyter fastai environment - the versions used in the challenge\n",
    "were fastai 1.0.55, torch 1.1.0, and torchvision 0.3.0.\n",
    "Note models trained and exported with these versions may be incompatible with later versions.\n",
    "\n",
    "### How to train the models and generate the Docker image starting from scratch:\n",
    "There are three types of models:\n",
    "- orientation (predict which of the 8 possible 90-degree orientations an x-ray image is in)\n",
    "- joint location (prediction where the joints used in RA scoring are on an image)\n",
    "- RA label prediction (predict joint narrowing and erosion scores)\n",
    "\n",
    "Each model uses the predictions of the previous ones.\n",
    "We start by predicting the orientation; this is then used to rotate/flip all images to the same\n",
    "orientation before feeding into the next models.\n",
    "This allows us to **merge left and right images** into the same models.\n",
    "We then predict joint locations and cut out a sub-image of the predicted joints.\n",
    "This is fed into the RA label prediction models.\n",
    "This allows us to **merge different joints** into the same models -\n",
    "for example, to train one model to predict a joint erosion score for any finger joint.\n",
    "\n",
    "To train the models and generate the Docker image starting from scratch, do the following:\n",
    "- Set imDirPath to the directory containing the training images and training.csv file (see code cell 3). Make sure there's an empty dock/ subdirectory under imDirPath for Docker related files.\n",
    "- Run the notebook up to the cell containing mark1.\n",
    "- Run the four sections of code between mark3 and mark4 - this will train and save/export the orientation and joint location models.\n",
    "- Restart the notebook and run up to the cell containing mark2. Then run the next two code cells to train all the RA label prediction models with current hyperparameter settings and update the dock/ subdirectory.\n",
    "- Use the Dockerfile and other files in the dock/ subdirectory to build the Docker image.\n",
    "\n",
    "### How to test performance and try out different hyperparameter settings:\n",
    "Do the first two steps above to set up the orientation and joint location models.\n",
    "Then, restart and run the notebook up to mark2.\n",
    "You can then add and run new cells to test cross-validation performance with different settings. For some examples, see the section **Examples of performance testing** at the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up paths and auto-generated scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_exported_scripts = set()\n",
    "# Indicate we're running from this notebook, not from one of\n",
    "# the Python scripts that can be auto-generated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction for_joints\n",
    "# Includes this cell in the auto-generated Python scripts\n",
    "# for prediction and the joint marking GUI.\n",
    "\n",
    "# import needed libraries:\n",
    "import collections\n",
    "import cv2\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from fastai import *\n",
    "from fastai import basic_train\n",
    "from fastai.vision import *\n",
    "import fastprogress\n",
    "\n",
    "import faiutils\n",
    "\n",
    "# set paths up for the notebook environment:\n",
    "imDirPath = Config().data_path()/'ratrain2'\n",
    "trainedModelPath = outDirPath = imDirPath\n",
    "mainCsvFName = 'training.csv'\n",
    "if ('for_prediction' in running_exported_scripts\n",
    "        and 'for_prediction_test' not in running_exported_scripts) :\n",
    "    # set paths up for the submitted Docker image:\n",
    "    imDirPath = Path('/test')\n",
    "    trainedModelPath = Path('/')\n",
    "    outDirPath = Path('/output')\n",
    "    mainCsvFName = 'template.csv'\n",
    "\n",
    "# auto-generate some scripts from this notebook:\n",
    "if not running_exported_scripts :\n",
    "    # standalone script to predict the test set, used for submit:\n",
    "    faiutils.exportFromNotebook('ra.ipynb',\n",
    "                    ['for_prediction','stepAllNew'], 'rapredAllNew.py')\n",
    "     # standalone script to predict the training set, used for local test:\n",
    "    faiutils.exportFromNotebook('ra.ipynb',\n",
    "                    ['for_prediction','stepAllNew','for_prediction_test'], 'rapredtestAllNew.py')\n",
    "     # script for import to the GUI used to mark joints:\n",
    "    faiutils.exportFromNotebook('ra.ipynb',\n",
    "                    ['for_joints'], 'rajoints.py')\n",
    "    faiutils.modelDirPath = modelDirPath = imDirPath/'models'\n",
    "else :\n",
    "    # turn off progress bar when running from auto-generated scripts\n",
    "    fastprogress.fastprogress.NO_BAR = True\n",
    "#     (basic_train.master_bar, basic_train.progress_bar\n",
    "#         ) = fastprogress.force_console_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imports below aren't required for prediction:\n",
    "\n",
    "import ipywidgets as widgets\n",
    "# from IPython.core.debugger import set_trace\n",
    "from IPython.display import display #, Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imDirPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data checking and massage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction for_joints\n",
    "\n",
    "# Functions to create lists of all individual feature labels:\n",
    "\n",
    "# Master lists of labels. XX is a placeholder that can be substituted with either J or E.\n",
    "footLabels = [f'F_mtp_XX__{suff}' for suff in ['5','4','3','2','1','ip']]\n",
    "handTopLabels = [f'H_pip_XX__{suff}' for suff in '5432']\n",
    "handMidLabels = [f'H_mcp_XX__{suff}' for suff in '54321'] + ['H_mcp_E__ip']\n",
    "wrist1_Labels = ['H_wrist_J__cmc5', 'H_wrist_J__cmc4', 'H_wrist_J__cmc3']\n",
    "wrist2_Labels = ['H_wrist_J__mna', 'H_wrist_J__radcar', 'H_wrist_J__capnlun']\n",
    "wrist3_Labels = ['H_wrist_E__nav', 'H_wrist_E__mul', 'H_wrist_E__mc1']\n",
    "wrist4_Labels = ['H_wrist_E__ulna', 'H_wrist_E__lunate', 'H_wrist_E__radius']\n",
    "handBotLabels = wrist1_Labels + wrist2_Labels + wrist3_Labels + wrist4_Labels\n",
    "\n",
    "def allMarkedJointLabels(HOrF) :\n",
    "    \"\"\"\n",
    "    List of all labels that have been marked on the XRays for hands, feet, or both.\n",
    "    This is a combination of the master lists above, except that the wrist labels\n",
    "    are consolidated into 4 points per wrist to save time on marking.\n",
    "    \"\"\"\n",
    "    if HOrF == 'F' :\n",
    "        return footLabels\n",
    "    elif HOrF == 'H' :\n",
    "        return handTopLabels + handMidLabels + [f'H_wrist{suff}' for suff in '1234']\n",
    "    elif HOrF == 'B' :\n",
    "        return allMarkedJointLabels('F')+allMarkedJointLabels('H')\n",
    "def initJointDF(df,HOrF) :\n",
    "    \"Initialize a dataframe for marking joint points.\"\n",
    "    df = df[['Patient_ID']].copy()\n",
    "    for label in allMarkedJointLabels(HOrF) :\n",
    "        df[label+'x'] = -1.0\n",
    "        df[label+'y'] = -1.0\n",
    "    return df\n",
    "\n",
    "markedLabelFrac = {}\n",
    "for label in footLabels :\n",
    "    markedLabelFrac[label] = 0.125\n",
    "markedLabelFrac['F_mtp_XX__1'] = markedLabelFrac['F_mtp_XX__ip'] = 0.15\n",
    "for label in handTopLabels :\n",
    "    markedLabelFrac[label] = 0.125\n",
    "for label in handMidLabels :\n",
    "    markedLabelFrac[label] = 0.125\n",
    "markedLabelFrac['H_mcp_XX__1'] = markedLabelFrac['H_mcp_E__ip'] = 0.15\n",
    "markedLabelFrac['H_wrist1'] = 0.25\n",
    "markedLabelFrac['H_wrist2'] = 0.25\n",
    "markedLabelFrac['H_wrist3'] = 0.25\n",
    "markedLabelFrac['H_wrist4'] = 0.25\n",
    "assert (set(markedLabelFrac.keys()) == set(allMarkedJointLabels('B'))\n",
    "       ), \"marked label fraction list mismatch\"\n",
    "\n",
    "# construct a map from all labels to the corresponding marked joint label\n",
    "labelToMarkedLabel = {}\n",
    "for labels,markedLabel in [(wrist1_Labels, 'H_wrist1'), (wrist2_Labels, 'H_wrist2'),\n",
    "                           (wrist3_Labels, 'H_wrist3'), (wrist4_Labels, 'H_wrist4')] :\n",
    "    for label in labels :\n",
    "        labelToMarkedLabel[label] = markedLabel\n",
    "for HOrF in 'HF' :\n",
    "    for markedLabel in allMarkedJointLabels(HOrF) :\n",
    "        if 'XX' in markedLabel :\n",
    "            labelToMarkedLabel[markedLabel.replace('XX','J')] = markedLabel\n",
    "            labelToMarkedLabel[markedLabel.replace('XX','E')] = markedLabel\n",
    "        elif '_J_' in markedLabel or '_E_' in markedLabel :\n",
    "            labelToMarkedLabel[markedLabel] = markedLabel\n",
    "\n",
    "# Functions that add a L or R prefix to the labels above, and substitute XX with E or F\n",
    "# to generate the full set of labels in the RA dataset.\n",
    "def add_LR_EJ(LOrR,EOrJ,labelList=[]) :\n",
    "    labelList = [label.replace('XX',EOrJ) for label in labelList]\n",
    "    return [LOrR+label for label in labelList\n",
    "            if '_'+EOrJ+'__' in label]\n",
    "FootLabelList = partial(add_LR_EJ, labelList=footLabels)\n",
    "HandTopList = partial(add_LR_EJ, labelList=handTopLabels)\n",
    "HandMidList = partial(add_LR_EJ, labelList=handMidLabels)\n",
    "HandBotList = partial(add_LR_EJ, labelList=handBotLabels)\n",
    "def HandLabelList(LOrR,EOrJ) :\n",
    "    return (HandTopList(LOrR,EOrJ) + HandMidList(LOrR,EOrJ) + HandBotList(LOrR,EOrJ))\n",
    "\n",
    "assert set(FootLabelList('','J') + FootLabelList('','E')\n",
    "           + HandLabelList('','J') + HandLabelList('','E')) == set(labelToMarkedLabel.keys()\n",
    "        ), \"label list mismatch\"\n",
    "\n",
    "def LabelList(LOrR,EOrJ,HOrF) :\n",
    "    if HOrF=='H' :\n",
    "        return HandLabelList(LOrR,EOrJ)\n",
    "    elif HOrF=='I' :\n",
    "        return HandTopList(LOrR,EOrJ)\n",
    "    elif HOrF=='J' :\n",
    "        return HandMidList(LOrR,EOrJ)\n",
    "    elif HOrF=='K' :\n",
    "        return HandBotList(LOrR,EOrJ)\n",
    "    elif HOrF=='F' :\n",
    "        return FootLabelList(LOrR,EOrJ)\n",
    "    else :\n",
    "        raise Exception('unknown label type '+str(HOrF))\n",
    "\n",
    "# Do a couple of checks that we got all those names right:\n",
    "\n",
    "print('checking total scores ... ',end='')\n",
    "maxScoreForLabel = {}\n",
    "for label in (FootLabelList('L','J')+FootLabelList('R','J')\n",
    "              + HandLabelList('L','J')+HandLabelList('R','J')) :\n",
    "    maxScoreForLabel[label] = 4\n",
    "for label in FootLabelList('L','E')+FootLabelList('R','E') :\n",
    "    maxScoreForLabel[label] = 10\n",
    "for label in HandLabelList('L','E')+HandLabelList('R','E') :\n",
    "    maxScoreForLabel[label] = 5\n",
    "assert sum(maxScoreForLabel.values())==448, \"total max score should be 448\"\n",
    "\n",
    "print('checking table columns ... ',end='')\n",
    "mainDF = pd.read_csv(imDirPath/mainCsvFName)\n",
    "mainColumns = set(mainDF.columns)\n",
    "for label in maxScoreForLabel.keys() :\n",
    "        assert label in mainColumns, \"missing data column \"+label\n",
    "        if not running_exported_scripts :\n",
    "            assert (mainDF[label].min()>=0\n",
    "                    and mainDF[label].max()<=maxScoreForLabel[label]),(\n",
    "                    \"value out of range for \"+label)\n",
    "print('done')\n",
    "\n",
    "def clipPredictions(df) :\n",
    "    \"Clip predicted label values in a dataframe to the valid range for each label.\"\n",
    "    for col,maxScore in maxScoreForLabel.items() :\n",
    "        df[col].clip(0,maxScore,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction for_joints\n",
    "\n",
    "def printCounts(label,df) :\n",
    "    \"Do a quick check of label distribution.\"\n",
    "    labelsWithCounts = sorted(collections.Counter(df[label]).items())\n",
    "    labelsWithCumCounts, cumCount = [],0\n",
    "    for v,count in labelsWithCounts :\n",
    "        cumCount += count\n",
    "        labelsWithCumCounts.append((v,count,cumCount))\n",
    "    print('counts for '+label+':',labelsWithCumCounts)\n",
    "printCounts('LF_mtp_J__5',mainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcValidationSliceMap(idList, seed=3141) :\n",
    "    \"Split the training set into n possible validation sets.\"\n",
    "    idList = list(idList)\n",
    "    random.Random(seed).shuffle(idList)\n",
    "    return dict((id,i) for i,id in enumerate(idList))\n",
    "\n",
    "validationSliceMap = calcValidationSliceMap(mainDF['Patient_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# validationSliceMap\n",
    "# {'UAB024': 0,\n",
    "#  'UAB278': 1,\n",
    "#  'UAB229': 2,\n",
    "#  'UAB405': 3,\n",
    "#  'UAB277': 4,\n",
    "#  'UAB451': 0,\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction for_joints\n",
    "\n",
    "def dupLR(df) :\n",
    "    \"Duplicate the training dataframe, adding 'L' and 'R' to one of each duplicated patient ID.\"\n",
    "    nPatients = len(df)\n",
    "    print('duplicating',nPatients,'rows ... ',end='')\n",
    "    dupDF = pd.concat([df,df]).reset_index(drop=True)\n",
    "    dupDF['Patient_ID'] += (nPatients*['L']+nPatients*['R'])\n",
    "    print('now',len(dupDF),'rows')\n",
    "    return dupDF\n",
    "\n",
    "def patientIDToIndex(df,patientID) :\n",
    "    return df[df.Patient_ID==patientID].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_joints\n",
    "\n",
    "dupDF = dupLR(mainDF)\n",
    "\n",
    "pd.concat([dupDF[0:2],dupDF[len(mainDF):len(mainDF)+2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction for_joints\n",
    "\n",
    "def splitDupCol(dupDF,newCol,convertToFloat=False) :\n",
    "    \"\"\"\n",
    "    Creates a new column with L/R removed and, for each duplicated patient ID with L or R added,\n",
    "    fills the new columns with the values from the L or R column in that row.\n",
    "\n",
    "    So, after calling dupLR and splitDupCol we go from:\n",
    "      PatientID  Lcol Rcol\n",
    "      Patient1    8    9\n",
    "    in the original main dataframe to:\n",
    "      PatientID  Lcol Rcol  col\n",
    "      Patient1L   8    9     8\n",
    "      Patient1R   8    9     9\n",
    "    \"\"\"\n",
    "    dupDF[newCol] = dupDF.apply(\n",
    "        lambda row : row[row['Patient_ID'][-1]+newCol],\n",
    "        axis=1)\n",
    "    if convertToFloat :\n",
    "        dupDF[newCol] = dupDF[newCol].astype(float)\n",
    "    printCounts(newCol,dupDF)\n",
    "def splitAllDupCols(dupDF) :\n",
    "    \"Call splitDupCol for all single joint feature columns.\"\n",
    "    for EOrJ,HOrF in itertools.product(\"EJ\",\"HF\") :\n",
    "        for newCol in LabelList('',EOrJ,HOrF) :\n",
    "            splitDupCol(dupDF,newCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export for_joints\n",
    "\n",
    "splitAllDupCols(dupDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupDF.loc[[0,1,len(mainDF),len(mainDF)+1]][\n",
    "    ['Patient_ID','LF_mtp_J__4','RF_mtp_J__4','F_mtp_J__4',\n",
    "     'LF_mtp_J__5','RF_mtp_J__5','F_mtp_J__5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction for_joints\n",
    "\n",
    "# Hyperparameters that affect training.\n",
    "# These are passed to the training functions (setupTraining, etc) using **\n",
    "defTrainingHPars = dict(\n",
    "    bs = 32,\n",
    "    imSize = 256,\n",
    "    trSeed=3141,\n",
    "    # trLabel = 'H_pip_E__5', # label or list of labels to train\n",
    "    # trHOrF = 'H',      # hand or foot\n",
    "    trJoints = None,\n",
    "    applyClahe = False,\n",
    "    myRemap=False, meanChange=0.05, stdDevChange=0.025,\n",
    "    vertStart=0.0, vertFrac=1.0, horizStart=0.0, horizFrac=1.0,\n",
    "    doOtsu=False,\n",
    "    gBlur=0, cv2Interp=False,\n",
    "    resizeToSize=None, returnNumpy=False,\n",
    "    topCropThresh=None,\n",
    "    padFrac=0.05,\n",
    "    arch=models.resnet34,\n",
    "    wd=0.01,\n",
    "    max_rotate=10.0,\n",
    "    max_lighting=0.2,\n",
    "    max_warp=0.2,\n",
    "    max_zoom=1.1,\n",
    "    do_flip=False, flip_vert=False,\n",
    "    borderType=cv2.BORDER_CONSTANT,\n",
    "    lin_ftrs=None,\n",
    "    ps=0.5,\n",
    "    nExtraCopiesFunc=None, # Function on row that returns number of extra copies\n",
    "                           # to make of that row for training.\n",
    "                           # ex: lambda row : 3 if row['Overall_Tol']>10 else 0\n",
    "    customHead=None, # if not None, should be a function returning\n",
    "                     # a custom head to supply to cnn_learner\n",
    "    addLayer=None, # if not None, should be a function returning\n",
    "                   # a layer to add to the NN, ex. for clipping output\n",
    "    cutLabel=None,\n",
    ")\n",
    "trainingHPars = dict(defTrainingHPars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special-purpose ImageList for the RA XRay images\n",
    "Flips the right-side images horizontally,\n",
    "and applies other transforms and modifications as specified by trainingHPars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction for_joints\n",
    "\n",
    "raGuiDispImSize=512\n",
    "\n",
    "def cv2fai(im) :\n",
    "    #im = 255-im\n",
    "    if len(im.shape) == 2 :\n",
    "        im = np.repeat(im[:,:,np.newaxis],3,axis=2)\n",
    "    #print(im.shape)\n",
    "    #im[:,:,0] = np.linspace(50,200,im.shape[1])\n",
    "    return Image(pil2tensor(im,dtype=np.float32).div_(255))\n",
    "def padIm(im,frac=0.1) :\n",
    "    height,width = im.shape\n",
    "    maxDim = max(height,width)\n",
    "    pad = int(maxDim*frac)\n",
    "    xPos = pad + int((maxDim-width)/2)\n",
    "    yPos = pad + int((maxDim-height)/2)\n",
    "    res = cv2.copyMakeBorder(im,yPos,maxDim+2*pad-(yPos+height),\n",
    "                             xPos,maxDim+2*pad-(xPos+width),\n",
    "                             trainingHPars['borderType'])\n",
    "    return res\n",
    "def findHistPosAbove(hist,thresh,pos,inc) :\n",
    "    while hist[pos] < thresh :\n",
    "        pos += inc\n",
    "    return pos\n",
    "def remapChan(im,outIm,low,high,toLow,toHigh) :\n",
    "    outIm[:,:] = im\n",
    "    outIm /= 255.0\n",
    "    outIm.clip(low,high,outIm)\n",
    "    outIm -= low\n",
    "    outIm *= ((toHigh-toLow)/(high-low))\n",
    "    outIm += toLow\n",
    "def myRemap(im, imStats=imagenet_stats, stdDevMult=1.5) :\n",
    "    hist = [int(h[0]) for h in cv2.calcHist([im],[0],None,[256],[0,256])]\n",
    "    low = findHistPosAbove(hist,20,0,1)/255.0\n",
    "    high = findHistPosAbove(hist,20,255,-1)/255.0\n",
    "    outIm = np.zeros(im.shape,np.float32)\n",
    "    means,stdDevs = imStats\n",
    "    chMean,chStdDev = means[1],stdDevs[1]\n",
    "    chMean += random.uniform(-trainingHPars['meanChange'],trainingHPars['meanChange'])\n",
    "    chStdDev += random.uniform(-trainingHPars['stdDevChange'],trainingHPars['stdDevChange'])\n",
    "    remapChan(im,outIm, low, high,\n",
    "                  max(0.0,chMean-stdDevMult*chStdDev),\n",
    "                  min(1.0,chMean+stdDevMult*chStdDev))\n",
    "    outIm *= 255.0\n",
    "    im[:,:] = outIm\n",
    "def cropIm(im,vertStart,vertFrac,horizStart,horizFrac) :\n",
    "    height,width = im.shape\n",
    "    return im[round(height*vertStart):round(height*(vertStart+vertFrac)),\n",
    "              round(width*horizStart):round(width*(horizStart+horizFrac))]\n",
    "def isBorder(diffs,thresh) :\n",
    "    return len(diffs)>10 and sorted(diffs)[-10]>=thresh\n",
    "def findTopBorder(im,thresh) :\n",
    "    height,width = im.shape\n",
    "    iStart = int(height*0.02)\n",
    "    for i in range(iStart,int(height*0.4)) :\n",
    "        diffs = np.abs((im[iStart]-im[i]).astype(np.int8))\n",
    "        if isBorder(diffs,thresh) :\n",
    "            return i\n",
    "    return int(height*0.05)\n",
    "def cropBorders(im) :\n",
    "    if trainingHPars['topCropThresh'] is not None :\n",
    "        im = im[findTopBorder(im,trainingHPars['topCropThresh']):]\n",
    "#     left = findLeftBorder(im,thresh)\n",
    "#     im = im[:,left:]\n",
    "#     right = findRightBorder(im,thresh)\n",
    "    return im\n",
    "\n",
    "def getRowJointPoints(jdf,ind) :\n",
    "    jpRow = jdf.loc[ind]\n",
    "    jpRow = [p*raGuiDispImSize for p in jpRow[1:]]  # rescale [0.0..1.0] to [0..raGuiDispImSize]\n",
    "    return tensor(jpRow).reshape((len(jpRow)//2,2))  # split into y,x coordinate pairs\n",
    "def getPatientJointPoints(jdf,patientID) :\n",
    "    return getRowJointPoints(jdf,patientIDToIndex(jdf,patientID))\n",
    "def getJointPoints(il,fPath) :\n",
    "    return getPatientJointPoints(il.jdf,os.path.basename(fPath))\n",
    "\n",
    "# show marked image points for debugging:\n",
    "def showPoints(il,patientID,scaledPts=None,figsize=(9,9)) :\n",
    "    if isinstance(patientID,int) :\n",
    "        ind = patientID\n",
    "    else :\n",
    "        ind = patientIDToIndex(il.jdf,patientID)\n",
    "    img = il[ind]\n",
    "    if scaledPts is not None :\n",
    "        if len(scaledPts.shape) == 1 :\n",
    "            scaledPts = scaledPts.reshape((scaledPts.shape[0]//2,2))\n",
    "        jpRow = ((scaledPts+1.0)/2.0)*raGuiDispImSize\n",
    "    else :\n",
    "        jpRow = getRowJointPoints(il.jdf,ind)\n",
    "    jpRow = jpRow.float()\n",
    "    #print(jpRow)\n",
    "    #print(FlowField(torch.Size([raGuiDispImSize,raGuiDispImSize]),jpRow))\n",
    "    points = ImagePoints(FlowField(torch.Size([raGuiDispImSize,raGuiDispImSize]),jpRow))\n",
    "    #print('image size:',img.size)\n",
    "    img.show(y=points,figsize=figsize)\n",
    "\n",
    "def getScaledChunkBounds(imSize,coord,frac=0.1,scaling='none') :\n",
    "    if scaling=='0..1' or scaling=='-1..1' :\n",
    "        if scaling == '-1..1' :\n",
    "            coord = (coord+1.0)/2.0\n",
    "        coord = int(coord*imSize)\n",
    "    chunkSize = int(imSize*frac/2.0)\n",
    "    coordStart = max(0,coord-chunkSize)\n",
    "    coordEnd = coordStart+2*chunkSize\n",
    "    if coordEnd>imSize :\n",
    "        coordEnd = imSize\n",
    "        coordStart = coordEnd-2*chunkSize\n",
    "    return coordStart,coordEnd\n",
    "def getImChunk(im,x,y,frac=0.1,scaling='none') :\n",
    "    imSize = im.shape[0]\n",
    "    xStart,xEnd = getScaledChunkBounds(imSize,x,frac,scaling)\n",
    "    yStart,yEnd = getScaledChunkBounds(imSize,y,frac,scaling)\n",
    "    return im[yStart:yEnd,xStart:xEnd]\n",
    "\n",
    "# draw box around image points for debugging:\n",
    "def markImChunk(im,x,y,frac=0.1,scaling='none') :\n",
    "    imSize = im.shape[0]\n",
    "    markSize = 3\n",
    "    xStart,xEnd = getScaledChunkBounds(imSize,x,frac,scaling)\n",
    "    yStart,yEnd = getScaledChunkBounds(imSize,y,frac,scaling)\n",
    "    im[yStart:yEnd,xStart:xStart+markSize] = 255\n",
    "    im[yStart:yEnd,xEnd-markSize:xEnd] = 255\n",
    "    im[yStart:yStart+markSize,xStart:xEnd] = 255\n",
    "    im[yEnd-markSize:yEnd,xStart:xEnd] = 255\n",
    "def markImChunks(im,pts,frac=0.1,scaling='-1..1') :\n",
    "    if pts is not None :\n",
    "        if len(pts.shape) == 1 :\n",
    "            pts = pts.reshape((pts.shape[0]//2,2))\n",
    "        for y,x in pts :\n",
    "            markImChunk(im,x.item(),y.item(),frac,scaling)\n",
    "\n",
    "def addYX(labels) :\n",
    "    return list(itertools.chain.from_iterable([label+'y',label+'x'] for label in labels))\n",
    "\n",
    "def doFlipMod(im,imageFlipMod) :\n",
    "    doFlip,nRot = imageFlipMod[0],imageFlipMod[1]\n",
    "    if nRot == '1' :\n",
    "        im = cv2.rotate(im,cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif nRot == '2' :\n",
    "        im = cv2.rotate(im,cv2.ROTATE_180)\n",
    "    elif nRot == '3' :\n",
    "        im = cv2.rotate(im,cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    if doFlip == 'F' :\n",
    "        im = im[::-1,:]\n",
    "    return im\n",
    "def undoFlipMod(im,imageFlipMod) :\n",
    "    doFlip,nRot = imageFlipMod[0],imageFlipMod[1]\n",
    "    if doFlip == 'F' :\n",
    "        im = im[::-1,:]\n",
    "    if nRot == '1' :\n",
    "        im = cv2.rotate(im,cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    elif nRot == '2' :\n",
    "        im = cv2.rotate(im,cv2.ROTATE_180)\n",
    "    elif nRot == '3' :\n",
    "        im = cv2.rotate(im,cv2.ROTATE_90_CLOCKWISE)\n",
    "    return im\n",
    "possibleFlipMods = ['F0','F1','F2','F3','N0','N1','N2','N3']\n",
    "def makeFlipModDF(df,nRepeats=1) :\n",
    "    imMods = nRepeats*list(itertools.chain.from_iterable(\n",
    "                        len(df)*[imMod] for imMod in possibleFlipMods))\n",
    "    dfPatients = df[['Patient_ID']]\n",
    "    dfPatients = (nRepeats*8)*[dfPatients]\n",
    "    df = pd.concat(dfPatients).reset_index(drop=True)\n",
    "    df['Patient_ID'] += ['$$' + imMod for imMod in imMods]\n",
    "    df['imMod'] = imMods\n",
    "    return df\n",
    "\n",
    "\n",
    "class RAImageList(ImageList) :\n",
    "    def open(self, fn) :\n",
    "        # print(fn)\n",
    "        forceHOrF = None\n",
    "        resMarkedLabel = trainingHPars['cutLabel']\n",
    "        if '*' in fn :\n",
    "            fn,resMarkedLabel = fn.split('*')\n",
    "            if 'XX' not in resMarkedLabel :\n",
    "                resMarkedLabel = labelToMarkedLabel[resMarkedLabel]\n",
    "            forceHOrF = resMarkedLabel[0]\n",
    "        if resMarkedLabel is not None :\n",
    "            resFrac = markedLabelFrac[resMarkedLabel]\n",
    "            predJDF = trainingHPars['predictedJointsDF']\n",
    "            ind = patientIDToIndex(predJDF,os.path.basename(fn))\n",
    "            resPredictedX = predJDF.at[ind,resMarkedLabel+'x']\n",
    "            resPredictedY = predJDF.at[ind,resMarkedLabel+'y']\n",
    "        imageFlipMod = None\n",
    "        if '$$' in fn :\n",
    "            fn,imageFlipMod = fn.split('$$')\n",
    "            flipModFunc = doFlipMod\n",
    "        else :\n",
    "            predODF = trainingHPars.get('predictedOrientDF')\n",
    "            if predODF is not None :\n",
    "                ind = patientIDToIndex(predODF,os.path.basename(fn))\n",
    "                imageFlipMod = predODF.at[ind,trainingHPars['trHOrF']+'Orient']\n",
    "                flipModFunc = undoFlipMod\n",
    "        LOrR = fn[-1]\n",
    "        fn = fn[:-1]\n",
    "        if fn[-1]=='-' :\n",
    "            fn = fn[:-1]\n",
    "        fn += ('-'+LOrR+trainingHPars['trHOrF']+'.jpg')\n",
    "        if os.path.isabs(fn) :\n",
    "            imPath = fn\n",
    "        else :\n",
    "            imPath = str(imDirPath/fn)\n",
    "        #print('fp',imPath)\n",
    "        if forceHOrF :\n",
    "            im = cv2.imread(re.sub('..jpg',forceHOrF+'.jpg',imPath),0)\n",
    "        else :\n",
    "            im = cv2.imread(imPath,0)\n",
    "        if LOrR=='R' :\n",
    "            im = im[:,::-1]\n",
    "        if trainingHPars['applyClahe'] :\n",
    "            if not hasattr(self,'clahe') :\n",
    "                self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16,16))\n",
    "        if trainingHPars['trJoints'] is not None :\n",
    "            if not hasattr(self,'jdf') :\n",
    "                self.jdf = pd.read_csv(trainingHPars['trHOrF']+'joints.csv')\n",
    "                markedLabelsYX = addYX(trainingHPars['trJoints'])\n",
    "                self.jdf = self.jdf[['Patient_ID']+markedLabelsYX]\n",
    "        if trainingHPars['doOtsu'] :\n",
    "            blur = cv2.GaussianBlur(im, (5,5), 0)\n",
    "            _,im = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        else :\n",
    "            im = cropBorders(im)\n",
    "            im = cropIm(im,trainingHPars['vertStart'],trainingHPars['vertFrac'],\n",
    "                        trainingHPars['horizStart'],trainingHPars['horizFrac'])\n",
    "            im = padIm(im,trainingHPars['padFrac'])\n",
    "            if imageFlipMod is not None :\n",
    "                im = flipModFunc(im,imageFlipMod)\n",
    "            if resMarkedLabel is not None :\n",
    "                im = getImChunk(im, resPredictedX, resPredictedY, frac=resFrac, scaling='-1..1')\n",
    "            if trainingHPars['myRemap'] :\n",
    "                myRemap(im)\n",
    "            if trainingHPars['applyClahe'] :\n",
    "                im = self.clahe.apply(im)\n",
    "            markImChunks(im,trainingHPars.get('debugMarkPts'),0.1)\n",
    "        gBlurSize = trainingHPars['gBlur']\n",
    "        if gBlurSize and im.shape[0]>trainingHPars['imSize']:\n",
    "            im = cv2.GaussianBlur(im,(gBlurSize,gBlurSize),0)\n",
    "        resizeToSize = trainingHPars['resizeToSize']\n",
    "        if resizeToSize is not None :\n",
    "            if isinstance(resizeToSize,int) :\n",
    "                resizeToSize = (resizeToSize,resizeToSize)\n",
    "            im = cv2.resize(im,resizeToSize)\n",
    "        elif trainingHPars['cv2Interp'] :\n",
    "            fromSize = im.shape[0]\n",
    "            toSize = trainingHPars['imSize']\n",
    "            interpType = cv2.INTER_AREA if fromSize>toSize else cv2.INTER_CUBIC\n",
    "            im = cv2.resize(im,(toSize,toSize),interpolation=interpType)\n",
    "        if trainingHPars['returnNumpy'] :\n",
    "            return im\n",
    "        return cv2fai(im)\n",
    "def RAImShow(fn,withPars=None) :\n",
    "    global trainingHPars\n",
    "    if withPars is not None :\n",
    "        trainingHPars = withPars\n",
    "    return RAImageList.from_folder(imDirPath).open(fn)\n",
    "def RAProcAll(toDir,withPars=None) :\n",
    "    global trainingHPars\n",
    "    if withPars is not None :\n",
    "        trainingHPars = withPars\n",
    "    destDirPath = Config().data_path()/toDir\n",
    "    if destDirPath.exists() :\n",
    "        print(destDirPath,'already exists!')\n",
    "        return\n",
    "    imNames = [imName.name for imName in imDirPath.ls()]\n",
    "    imNames = [imName[:-5].replace('-','')\n",
    "               for imName in imNames\n",
    "               if imName.endswith(trainingHPars['trHOrF']+'.jpg')]\n",
    "    il = RAImageList.from_df(pd.DataFrame(imNames),imDirPath)\n",
    "    destDirPath.mkdir()\n",
    "    for fp,im in zip(il.items,il) :\n",
    "        imFName = os.path.basename(fp[:-1]+'-'+fp[-1]+trainingHPars['trHOrF']+'.jpg')\n",
    "        print(fp,'->',destDirPath/imFName,im)\n",
    "        im.save(destDirPath/imFName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various experiments with customizing the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "# The default cnn_learner head:\n",
    "# BatchNorm1d          [1024]               2,048      True      \n",
    "# ______________________________________________________________________\n",
    "# Dropout              [1024]               0          False     \n",
    "# ______________________________________________________________________\n",
    "# Linear               [512]                524,800    True      \n",
    "# ______________________________________________________________________\n",
    "# ReLU                 [512]                0          False     \n",
    "# ______________________________________________________________________\n",
    "# BatchNorm1d          [512]                1,024      True      \n",
    "# ______________________________________________________________________\n",
    "# Dropout              [512]                0          False     \n",
    "# ______________________________________________________________________\n",
    "# Linear  \n",
    "\n",
    "def customHead1(endDepth,endSize,hiddenSize=256) :\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(endDepth,hiddenSize,1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(hiddenSize),\n",
    "        nn.Conv2d(hiddenSize,2,1),\n",
    "        Flatten(),\n",
    "        nn.Linear(endSize*endSize*2,256),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.Linear(256,2)\n",
    "    )\n",
    "class scaledSigma(nn.Module) :\n",
    "    def __init__(self, lower, upper, slop=None) :\n",
    "        \"\"\"scale output to lower-slop .. upper+slop using sigmoid\"\"\"\n",
    "        super(scaledSigma,self).__init__()\n",
    "        if slop is None :\n",
    "            slop = 0.1*(upper-lower)\n",
    "        self.yLow = lower-slop\n",
    "        self.yFactor = 2*slop + (upper-lower)\n",
    "        self.lower, self.upper, self.slop = lower,upper,slop\n",
    "    def forward(self, input) :\n",
    "        return self.yLow + self.yFactor*torch.sigmoid(input)\n",
    "class clipOutput(nn.Module) :\n",
    "    def __init__(self, lower, upper) :\n",
    "        \"\"\"clip output to lower .. upper\"\"\"\n",
    "        super(clipOutput,self).__init__()\n",
    "        self.lower, self.upper = lower, upper\n",
    "    def forward(self, input) :\n",
    "        return torch.clamp(input,self.lower,self.upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level functions for training and prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "def clearCurGlobals() :\n",
    "    global il,data,learn\n",
    "    il = data = learn = None\n",
    "def setupTraining(valSliceNo=0, nExtraCopiesFunc=None,\n",
    "                  label_cls=FloatList, metrics=None, df=None, nCrossValSlices=5, **kwargs) :\n",
    "    global trainingHPars,il,data,learn\n",
    "    if df is None :\n",
    "        df = dupDF\n",
    "    print(f'CV slice {valSliceNo}/{nCrossValSlices}')\n",
    "    trainingHPars = dict(defTrainingHPars,**kwargs)\n",
    "    clearCurGlobals()\n",
    "    if valSliceNo is not None :\n",
    "        df = df.copy()\n",
    "        df['isValid'] = df['Patient_ID'].apply(lambda x :\n",
    "                (validationSliceMap[x.split('*')[0].split('$$')[0][:-1]] % nCrossValSlices) == valSliceNo)\n",
    "        notInValidSet = ~(df['isValid'])\n",
    "    if nExtraCopiesFunc is not None :\n",
    "        nExtraCopies = df.apply(nExtraCopiesFunc, axis=1)\n",
    "        i = nExtraRows = 0\n",
    "        extraRowsList = []\n",
    "        while True :\n",
    "            if valSliceNo is not None :\n",
    "                extraRows = df[(nExtraCopies>i) & notInValidSet]\n",
    "            else :\n",
    "                extraRows = df[(nExtraCopies>i)]\n",
    "            if len(extraRows) == 0 :\n",
    "                break\n",
    "            extraRowsList.append(extraRows)\n",
    "            nExtraRows += len(extraRows)\n",
    "            i += 1\n",
    "        print('adding',nExtraRows,'extra rows ... ',end='')\n",
    "        df = pd.concat([df]+extraRowsList).reset_index(drop=True)\n",
    "        print(len(df),'total rows')\n",
    "    il = RAImageList.from_df(df,imDirPath)\n",
    "    img = il[0]  # will initialize il.clahe and il.jdf if needed\n",
    "    if valSliceNo is None :\n",
    "        ilsp = il.split_none()\n",
    "    else :\n",
    "        ilsp = il.split_from_df('isValid')\n",
    "    doingJointTraining = (trainingHPars['trJoints'] is not None)\n",
    "    if doingJointTraining :\n",
    "        ilspLab = ilsp.label_from_func(partial(getJointPoints,il),label_cls=PointsLabelList)\n",
    "    else :\n",
    "        ilspLab = ilsp.label_from_df(trainingHPars['trLabel'],label_cls=label_cls)\n",
    "    tfms = get_transforms(do_flip=trainingHPars['do_flip'],flip_vert=trainingHPars['flip_vert'],\n",
    "                         max_rotate=trainingHPars['max_rotate'],\n",
    "                         max_lighting=trainingHPars['max_lighting'],\n",
    "                         max_warp=trainingHPars['max_warp'],\n",
    "                         max_zoom=trainingHPars['max_zoom'],\n",
    "                         )\n",
    "    data = ilspLab.transform((tfms[0],[]) if doingJointTraining else tfms,\n",
    "                             tfm_y=doingJointTraining,\n",
    "                             size=trainingHPars['imSize']\n",
    "                            ).databunch(bs=trainingHPars['bs'],\n",
    "                                        num_workers=0,\n",
    "                            ).normalize(imagenet_stats)\n",
    "    faiutils.randomSeedForTraining(trainingHPars['trSeed'])\n",
    "    if trainingHPars['customHead'] is not None :\n",
    "        custom_head = trainingHPars['customHead']()\n",
    "    else :\n",
    "        custom_head = None\n",
    "    learn = cnn_learner(data,trainingHPars['arch'],\n",
    "                        wd=trainingHPars['wd'],\n",
    "                        lin_ftrs=trainingHPars['lin_ftrs'],\n",
    "                        ps=trainingHPars['ps'],\n",
    "                        custom_head=custom_head,\n",
    "                        metrics=metrics,\n",
    "                       )\n",
    "    if trainingHPars['addLayer'] is not None :\n",
    "        learn.model[1].add_module('addLayer',trainingHPars['addLayer']())\n",
    "    print('loss',learn.loss_func)\n",
    "    return learn,data\n",
    "def tryTraining(nEpochs=20, lr=5e-3, **kwargs) :\n",
    "    setupTraining(**kwargs)\n",
    "    learn.fit_one_cycle(nEpochs,lr)\n",
    "    val_losses = learn.recorder.val_losses\n",
    "    # print(val_losses)\n",
    "    if val_losses and val_losses[0] is not None :\n",
    "        return {'final':val_losses[-1], 'min':min(val_losses)}\n",
    "    return {}\n",
    "def tryCrossVal(nEpochs=20, lr=5e-3, nCrossValSlices=5, **kwargs) :\n",
    "    res = {'final':[], 'min':[]}\n",
    "    for i in range(nCrossValSlices) :\n",
    "        resI = tryTraining(nEpochs, lr, valSliceNo=i, nCrossValSlices=nCrossValSlices, **kwargs)\n",
    "        print(resI)\n",
    "        for k in res.keys() :\n",
    "            res[k].append(resI[k])\n",
    "    for k in res.keys() :\n",
    "        res[k] = sum(res[k])/len(res[k])\n",
    "    return res\n",
    "def doFullTraining(exportFName=None, nEpochs=20, lr=5e-3, **kwargs) :\n",
    "    print(exportFName,nEpochs,lr,kwargs)\n",
    "    tryTraining(nEpochs, lr, valSliceNo=None, **kwargs)\n",
    "    learn.export(exportFName,destroy=True)\n",
    "    clearCurGlobals()\n",
    "def predictUsingExportedModel(exportFDir, exportFName, df, testImDirPath, labelList, hPars) :\n",
    "    global trainingHPars\n",
    "    trainingHPars = dict(defTrainingHPars,**hPars)\n",
    "    learn = load_learner(path=exportFDir, file=exportFName,\n",
    "                         num_workers=0)\n",
    "    learn.data.add_test(RAImageList.from_df(df, testImDirPath),\n",
    "                        tfms=None, tfm_y=False)\n",
    "    print('loaded batch size',learn.data.batch_size)\n",
    "    #learn.data.batch_size = 4\n",
    "    preds,_ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "    print('output shape',preds.shape)\n",
    "    learn.destroy()\n",
    "    del learn\n",
    "    gc.collect()\n",
    "    if labelList is not None :\n",
    "        df[labelList] = preds\n",
    "    return preds\n",
    "# def predictUsingExportedModel2(exportFDir, exportFName, df, testImDirPath, labelList, hPars) :\n",
    "#     global trainingHPars\n",
    "#     trainingHPars = dict(defTrainingHPars,**hPars)\n",
    "#     learn = load_learner(path=exportFDir, file=exportFName,\n",
    "#                          test=RAImageList.from_df(df, testImDirPath),\n",
    "#                          num_workers=0)\n",
    "#     print('loaded batch size',learn.data.batch_size)\n",
    "#     #learn.data.batch_size = 4\n",
    "#     preds,_ = learn.get_preds(ds_type=DatasetType.Test)\n",
    "#     print('output shape',preds.shape)\n",
    "#     learn.destroy()\n",
    "#     del learn\n",
    "#     gc.collect()\n",
    "#     if df is not None :\n",
    "#         df[labelList] = preds\n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "# current hyperparameter settings for training joint location models:\n",
    "JLocTrainPars = dict(defTrainingHPars,\n",
    "                     resizeToSize=raGuiDispImSize,\n",
    "                     max_rotate=5.0,max_zoom=1.05,max_warp=0.1,\n",
    "                     wd=0.1,\n",
    "                    )\n",
    "FJLocTrainPars = dict(JLocTrainPars, trHOrF='F', trJoints=allMarkedJointLabels('F'))\n",
    "HJLocTrainPars = dict(JLocTrainPars, trHOrF='H', trJoints=allMarkedJointLabels('H'))\n",
    "\n",
    "JPModelLists = {\n",
    "    'F': [('FJoint_40epoch_0_04_wd_0_1', allMarkedJointLabels('F'), FJLocTrainPars)],\n",
    "    'H': [('HJoint_40epoch_0_04_wd_0_1', allMarkedJointLabels('H'), HJLocTrainPars)],\n",
    "}\n",
    "JPModelLists['B'] = JPModelLists['F']+JPModelLists['H']\n",
    "\n",
    "def predictJoints(df,HOrF,predictedOrientDF=None) :\n",
    "    predictedJointsDF = initJointDF(df,HOrF)\n",
    "    for modelName,markedLabelList,hPars in JPModelLists[HOrF] :\n",
    "        predictUsingExportedModel(trainedModelPath, modelName+'.pkl', predictedJointsDF,\n",
    "                                  imDirPath, addYX(markedLabelList),\n",
    "                                  dict(hPars, trJoints=None, predictedOrientDF=predictedOrientDF))\n",
    "    return predictedJointsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orientation prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "# current hyperparameter settings for training orientation models:\n",
    "OrientTrainPars = dict(defTrainingHPars,\n",
    "                       label_cls=None, metrics=accuracy, trLabel='imMod', imSize=224,\n",
    "                       wd=0.1,\n",
    "                    )\n",
    "FOrientTrainPars = dict(OrientTrainPars, trHOrF='F')\n",
    "HOrientTrainPars = dict(OrientTrainPars, trHOrF='H')\n",
    "\n",
    "OrientModelLists = {\n",
    "    'F': [('FOrient_3epoch_0_01_wd_0_1', 'FOrient', FOrientTrainPars)],\n",
    "    'H': [('HOrient_3epoch_0_01_wd_0_1', 'HOrient', HOrientTrainPars)],\n",
    "}\n",
    "OrientModelLists['B'] = OrientModelLists['F']+OrientModelLists['H']\n",
    "\n",
    "def predictOrient(df,HOrF) :\n",
    "    predictedOrientDF = df[['Patient_ID']].copy()\n",
    "    for modelName,resLabel,hPars in OrientModelLists[HOrF] :\n",
    "        preds = predictUsingExportedModel(trainedModelPath, modelName+'.pkl', predictedOrientDF,\n",
    "                                          imDirPath, None, hPars)\n",
    "        preds = [possibleFlipMods[pred.item()] for pred in preds.argmax(dim=1)]\n",
    "        predictedOrientDF[resLabel] = preds\n",
    "        print('orientations',resLabel,set(preds))\n",
    "    return predictedOrientDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orpredDF = predictOrient(dupDF,'B')\n",
    "# set(orpredDF[:367]['HOrient']),set(orpredDF[:367]['FOrient']),set(orpredDF[367:]['HOrient']),set(orpredDF[367:]['FOrient'])\n",
    "# orpredDF.at[366,'FOrient'],orpredDF.at[367,'HOrient']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using predicted joints for training and label prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "def combDFToPointDF(combDF,labels) :\n",
    "    pointDFList = []\n",
    "    if isinstance(labels,str) : labels = [labels]\n",
    "    for label in labels :\n",
    "        pointDFList.append(combDF[['Patient_ID']].copy())\n",
    "        pointDFList[-1]['Patient_ID'] += ('*'+label)\n",
    "        if 'XX' in label :\n",
    "            pointDFList[-1]['resultJ'] = combDF[label.replace('XX','J')]\n",
    "            pointDFList[-1]['resultE'] = combDF[label.replace('XX','E')]\n",
    "        else :\n",
    "            pointDFList[-1]['result'] = combDF[label]\n",
    "    return pd.concat(pointDFList).reset_index(drop=True)\n",
    "def insertPointDF(pointDF,combDF) :\n",
    "    n = len(combDF)\n",
    "    for pos in range(0,len(pointDF),n) :\n",
    "        labelDF = pointDF[pos:pos+n].copy().reset_index(drop=True)\n",
    "        curLabel = labelDF['Patient_ID'][0].split('*')[1]\n",
    "        if 'XX' in curLabel :\n",
    "            combDF[curLabel.replace('XX','J')] = labelDF['resultJ']\n",
    "            combDF[curLabel.replace('XX','E')] = labelDF['resultE']\n",
    "        else :\n",
    "            combDF[curLabel] = labelDF['result']\n",
    "\n",
    "def trainUsingPredictedJoints(trainingFunc, trLabel=None, **kwargs) :\n",
    "    pointDF = combDFToPointDF(dupDF,trLabel)\n",
    "    return trainingFunc(trLabel='result', df=pointDF,\n",
    "                        predictedJointsDF=predictedDupDFJoints, **kwargs)\n",
    "def trainUsingPredictedJoints2(trainingFunc, trLabel=None, **kwargs) :\n",
    "    pointDF = combDFToPointDF(dupDF,trLabel)\n",
    "    return trainingFunc(trLabel=['resultJ','resultE'], df=pointDF,\n",
    "                        predictedJointsDF=predictedDupDFJoints, **kwargs)\n",
    "\n",
    "def trainUsingCutJoint(trainingFunc, **kwargs) :\n",
    "    return trainingFunc(predictedJointsDF=predictedDupDFJoints, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "def pearsonr(x,y) :\n",
    "    # print(x.shape,y.shape)\n",
    "    x,y = x.flatten(),y.flatten()\n",
    "    meanx = torch.mean(x)\n",
    "    meany = torch.mean(y)\n",
    "    xm = x.sub(meanx)\n",
    "    ym = y.sub(meany)\n",
    "    rnum = xm.dot(ym)\n",
    "    rden = torch.norm(xm,2) * torch.norm(ym,2)\n",
    "    return rnum/rden\n",
    "class PearsonR(Callback) :\n",
    "    def on_epoch_begin(self,**kwargs) :\n",
    "        self.preds = []\n",
    "        self.target = []\n",
    "    def on_batch_end(self, last_output, last_target, **kwargs) :\n",
    "        self.preds.append(last_output.detach().clone())\n",
    "        self.target.append(last_target.detach().clone())\n",
    "    def on_epoch_end(self, last_metrics, **kwargs) :\n",
    "        self.preds = torch.cat(self.preds)\n",
    "        self.target = torch.cat(self.target)\n",
    "        return add_metrics(last_metrics, pearsonr(self.preds, self.target).item())\n",
    "    def __repr__(self) :\n",
    "        return 'PearsonR()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "# current hyperparameter settings for training RA label prediction models:\n",
    "newHPars = dict(arch=models.densenet201,\n",
    "                addLayer=partial(scaledSigma,0.0,4.0,0.4),\n",
    "                imSize=224,\n",
    "                wd=0.075,\n",
    "                bs=24,\n",
    "                nExtraCopiesFunc = lambda row : 0 if row[1]==0.0 else 1,\n",
    "                nEpochs=30,\n",
    "                lr=0.01,\n",
    "                label_cls=FloatList,\n",
    "                metrics=[PearsonR()],\n",
    "                #myRemap=True,\n",
    "                #applyClahe=True,\n",
    ")\n",
    "\n",
    "pointHPars = {\n",
    "    'footJ' : dict(newHPars, trHOrF='F',\n",
    "            trLabel=['F_mtp_J__5','F_mtp_J__4','F_mtp_J__3','F_mtp_J__2','F_mtp_J__1',\n",
    "                     'F_mtp_J__ip'],\n",
    "    ),\n",
    "    'footE' : dict(newHPars, trHOrF='F',\n",
    "            trLabel=['F_mtp_E__5','F_mtp_E__4','F_mtp_E__3','F_mtp_E__2','F_mtp_E__1',\n",
    "                     'F_mtp_E__ip'],\n",
    "            addLayer=partial(scaledSigma,0.0,10.0,0.5),\n",
    "            nEpochs=50, lr=0.005, gBlur=3,\n",
    "    ),\n",
    "    'handTopJ' : dict(newHPars, trHOrF='H',\n",
    "            trLabel=['H_pip_J__5','H_pip_J__4','H_pip_J__3','H_pip_J__2'],\n",
    "    ),\n",
    "    'handMidJ' : dict(newHPars, trHOrF='H',\n",
    "            trLabel=['H_mcp_J__5','H_mcp_J__4','H_mcp_J__3','H_mcp_J__2','H_mcp_J__1'],\n",
    "    ),\n",
    "    'handTopAndMidE' : dict(newHPars, trHOrF='H',\n",
    "            trLabel=['H_pip_E__5','H_pip_E__4','H_pip_E__3','H_pip_E__2',\n",
    "                     'H_mcp_E__5','H_mcp_E__4','H_mcp_E__3','H_mcp_E__2','H_mcp_E__1',\n",
    "                     'H_mcp_E__ip'],\n",
    "            addLayer=partial(scaledSigma,0.0,5.0,0.5),\n",
    "            nEpochs=50, lr=0.005, gBlur=3,\n",
    "    ),\n",
    "}\n",
    "\n",
    "cutHPars = {\n",
    "    'wrist1' : dict(newHPars, trHOrF='H', trLabel=wrist1_Labels, cutLabel='H_wrist1',\n",
    "            nExtraCopiesFunc = lambda row : 1 if any(row[label]>0.0 for label in wrist1_Labels) else 0,\n",
    "    ),\n",
    "    'wrist2' : dict(newHPars, trHOrF='H', trLabel=wrist2_Labels, cutLabel='H_wrist2',\n",
    "            nExtraCopiesFunc = lambda row : 1 if any(row[label]>0.0 for label in wrist2_Labels) else 0,\n",
    "    ),\n",
    "    'wrist3' : dict(newHPars, trHOrF='H', trLabel=wrist3_Labels, cutLabel='H_wrist3',\n",
    "            nExtraCopiesFunc = lambda row : 1 if any(row[label]>0.0 for label in wrist3_Labels) else 0,\n",
    "            addLayer=partial(scaledSigma,0.0,5.0,0.5),\n",
    "    ),\n",
    "    'wrist4' : dict(newHPars, trHOrF='H', trLabel=wrist4_Labels, cutLabel='H_wrist4',\n",
    "            nExtraCopiesFunc = lambda row : 1 if any(row[label]>0.0 for label in wrist4_Labels) else 0,\n",
    "            addLayer=partial(scaledSigma,0.0,5.0,0.5),\n",
    "    ),\n",
    "}\n",
    "\n",
    "def fullTrainAllNew() :\n",
    "    # train and export all models\n",
    "    for name,hPars in pointHPars.items() :\n",
    "        trainUsingPredictedJoints(doFullTraining,**dict(hPars, exportFName=name+'_model'+'.pkl'))\n",
    "    for name,hPars in cutHPars.items() :\n",
    "        trainUsingCutJoint(doFullTraining,**dict(hPars, exportFName=name+'_model'+'.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Docker directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyFToDocker(fromFName, dockerFileStrs, dockerDir, fromDir='.', toFName=None) :\n",
    "    if toFName is None :\n",
    "        toFName = fromFName\n",
    "    shutil.copyfile(os.path.join(fromDir,fromFName),os.path.join(dockerDir,toFName))\n",
    "    if dockerFileStrs is not None :\n",
    "        dockerFileStrs.append(f'COPY {toFName} /{toFName}')\n",
    "\n",
    "def updateDockerDir(dockerDir) :\n",
    "    dockerFileStrs = [\n",
    "\"\"\"FROM paperspace/fastai@sha256:81e470fb2e55509487541a4303f3f95a7d5e3c428d6ef925c9e50c039751e6ed\n",
    "\n",
    "ENV PATH=$PATH:/opt/conda/bin/\n",
    "ENV USER fastai\n",
    "WORKDIR /\n",
    "RUN conda uninstall -n fastai -y fastai pytorch torchvision\n",
    "RUN conda install -n fastai -c pytorch -c fastai -y fastai=1.0.55 pytorch=1.1.0 torchvision=0.3.0\n",
    "RUN conda install -n fastai -y opencv\n",
    "\n",
    "# Required: Create /train /test and /output directories \n",
    "RUN mkdir /train\n",
    "RUN mkdir /test\n",
    "RUN mkdir /output\n",
    "\"\"\"]\n",
    "    for name,_,_ in JPModelLists['B'] :\n",
    "        copyFToDocker(name+'.pkl',dockerFileStrs,dockerDir,str(trainedModelPath))\n",
    "    for name,_,_ in OrientModelLists['B'] :\n",
    "        copyFToDocker(name+'.pkl',dockerFileStrs,dockerDir,str(trainedModelPath))\n",
    "    for name,_ in pointHPars.items() :\n",
    "        copyFToDocker(name+'_model'+'.pkl',dockerFileStrs,dockerDir,str(trainedModelPath))\n",
    "    for name,_ in cutHPars.items() :\n",
    "        copyFToDocker(name+'_model'+'.pkl',dockerFileStrs,dockerDir,str(trainedModelPath))\n",
    "    copyFToDocker('rapredAllNew.py',dockerFileStrs,dockerDir)\n",
    "    copyFToDocker('faiutils.py',dockerFileStrs,dockerDir)\n",
    "    copyFToDocker('runallnew.sh',dockerFileStrs,dockerDir,toFName='run.sh')\n",
    "    copyFToDocker('build.sh',None,dockerDir)\n",
    "    copyFToDocker('eval.sh',None,dockerDir)\n",
    "    copyFToDocker('term.sh',None,dockerDir)\n",
    "    dockerFileStrs.append(\"\"\"\n",
    "# Make model and runfiles executable \n",
    "RUN chmod 775 /run.sh\n",
    "\n",
    "# This is for the virtualenv defined above, if not using a virtualenv, this is not necessary\n",
    "RUN chmod 755 /root #to make virtualenv accessible to singularity user\n",
    "\n",
    "# Required: define an entrypoint. run.sh will run the model for us, but in a different configuration\n",
    "# you could simply call the model file directly as an entrypoint \n",
    "ENTRYPOINT [\"/bin/bash\", \"/run.sh\"]\n",
    "\"\"\")\n",
    "    with open(os.path.join(dockerDir,'Dockerfile'),'w') as f :\n",
    "        f.write('\\n'.join(dockerFileStrs))\n",
    "    print('updated',dockerDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "\n",
    "def initPredSplitDF(testCsvPath) :\n",
    "    predSplitDF = dupLR(pd.read_csv(testCsvPath))\n",
    "    splitAllDupCols(predSplitDF)\n",
    "    return predSplitDF\n",
    "def writeFinalDF(testCsvPath, predSplitDF, outCsvPath) :\n",
    "    outDF = pd.read_csv(testCsvPath)\n",
    "    nPatients = len(outDF)\n",
    "    predSplitDFL = predSplitDF[:nPatients].copy().reset_index(drop=True)\n",
    "    predSplitDFR = predSplitDF[nPatients:].copy().reset_index(drop=True)\n",
    "    allLabels = {'E':[], 'J':[]}\n",
    "    for EOrJ in \"EJ\" :\n",
    "        for labelListFunc in [HandLabelList,FootLabelList] :\n",
    "            lLabels,rLabels,labels = [labelListFunc(pref,EOrJ) for pref in ['L','R','']]\n",
    "            outDF[lLabels] = predSplitDFL[labels]\n",
    "            outDF[rLabels] = predSplitDFR[labels]\n",
    "            allLabels[EOrJ].extend(lLabels+rLabels)\n",
    "    clipPredictions(outDF)\n",
    "    outDF['Overall_erosion'] = outDF[allLabels['E']].sum(axis='columns')\n",
    "    outDF['Overall_narrowing'] = outDF[allLabels['J']].sum(axis='columns')\n",
    "    outDF['Overall_Tol'] = outDF[['Overall_erosion','Overall_narrowing']].sum(axis='columns')\n",
    "    outDF.to_csv(outCsvPath,index=False)\n",
    "def predictAllNew(testCsvPath, testImDirPath, outCsvPath) :\n",
    "    predSplitDF = initPredSplitDF(testCsvPath)\n",
    "    predictedOrientDF = predictOrient(predSplitDF,'B')\n",
    "    predictedJointsDF = predictJoints(predSplitDF,'B',\n",
    "                                       predictedOrientDF=predictedOrientDF)\n",
    "    # predict all labels\n",
    "    for name,hPars in pointHPars.items() :\n",
    "        pointDF = combDFToPointDF(predSplitDF,hPars['trLabel'])\n",
    "        predictUsingExportedModel(trainedModelPath, name+'_model'+'.pkl', pointDF,\n",
    "                                  testImDirPath, ['result'],\n",
    "                                  dict(hPars, trLabel=['result'],\n",
    "                                       predictedOrientDF=predictedOrientDF,\n",
    "                                       predictedJointsDF=predictedJointsDF))\n",
    "        insertPointDF(pointDF,predSplitDF)\n",
    "    for name,hPars in cutHPars.items() :\n",
    "        predictUsingExportedModel(trainedModelPath, name+'_model'+'.pkl', predSplitDF,\n",
    "                                  testImDirPath, hPars['trLabel'],\n",
    "                                  dict(hPars,\n",
    "                                       predictedOrientDF=predictedOrientDF,\n",
    "                                       predictedJointsDF=predictedJointsDF))\n",
    "    writeFinalDF(testCsvPath, predSplitDF, outCsvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for_prediction\n",
    "if 'stepAllNew' in running_exported_scripts :\n",
    "    predictAllNew(imDirPath/mainCsvFName, imDirPath, outDirPath/'predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging/tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View examples with worst training and validation errors\n",
    "# Used to manually check the worst error cases for image regression models.\n",
    "def getMaxMSE(learn, n=20, ds_type=DatasetType.Valid) :\n",
    "    preds,target = learn.get_preds(ds_type=ds_type)\n",
    "    preds = preds.flatten()\n",
    "    errs = sorted(zip(range(len(preds)),(preds-target)**2,preds,target),\n",
    "                  key=lambda x : x[1], reverse=True)\n",
    "    return [(learn.data.dl(ds_type).dataset[i][0], err.item(), pred.item(), targ.item())\n",
    "           for i,err,pred,targ in errs[:n]]\n",
    "    return errs\n",
    "def plotMaxMSEs(learn, ns=3, figsize=(9,9), ds_type=DatasetType.Valid) :\n",
    "    maxMSEs = getMaxMSE(learn, ns*ns, ds_type)\n",
    "    fig,axes = plt.subplots(ns, ns, figsize=figsize)\n",
    "    for i,(im,err,pred,targ) in enumerate(maxMSEs) :\n",
    "        im.show(ax=axes.flat[i],\n",
    "               title=f'{\"^\" if pred>targ else \"v\"} pred {pred:.3f}, act {targ:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymse(x,y) :\n",
    "    return ((x.flatten()-y.flatten())**2).mean()\n",
    "def get_mse(learn) :\n",
    "    pred,target = learn.get_preds()\n",
    "    return mymse(pred,target)\n",
    "def get_mse_and_pearson(learn,col=None) :\n",
    "    pred,target = learn.get_preds()\n",
    "    # print(pred.shape,target.shape)\n",
    "    if col is not None :\n",
    "        pred = pred[:,col]\n",
    "        target = target[:,col]\n",
    "    return mymse(pred,target), pearsonr(pred,target)\n",
    "\n",
    "def get_TTA_mse(learn, nRuns=4, beta=0.4) :\n",
    "    pred,target = learn.get_preds()\n",
    "    saveTfms = learn.data.valid_ds.tfms\n",
    "    learn.data.valid_ds.tfms = learn.data.train_ds.tfms\n",
    "    for i in range(nRuns) :\n",
    "        p,_ = learn.get_preds()\n",
    "        if i == 0 :\n",
    "            ttaPred = p\n",
    "        else :\n",
    "            ttaPred += p\n",
    "        print(i,'-',((p.flatten()-target.flatten())**2).mean())\n",
    "    learn.data.valid_ds.tfms = saveTfms\n",
    "    ttaPred /= nRuns\n",
    "    pred = pred*beta + ttaPred*(1-beta)\n",
    "    return mymse(pred,target), pearsonr(pred,target)\n",
    "\n",
    "def getMetsTr(name,suff,valSliceNo=0,**kwargs) :\n",
    "    trainUsingPredictedJoints(setupTraining,**dict(pointHPars[name],valSliceNo=valSliceNo,**kwargs))\n",
    "    learn.load(name+suff)\n",
    "    return get_mse_and_pearson(learn)\n",
    "def getMetsCu(name,suff,**kwargs) :\n",
    "    trainUsingCutJoint(setupTraining,**dict(cutHPars[name],**kwargs))\n",
    "    learn.load(name+suff)\n",
    "    return get_mse_and_pearson(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPredictionsDotCsv(predFPath=None) :\n",
    "    \"\"\"Check the predictions.csv file produced by rapredtestAllNew.py\"\"\"\n",
    "    if predFPath is None :\n",
    "        predFPath = imDirPath/'predictions.csv'\n",
    "    testPred = pd.read_csv(predFPath)\n",
    "    testPred = testPred[testPred.columns[1:]]\n",
    "    testTarget = pd.read_csv(imDirPath/'training.csv')\n",
    "    testTarget = testTarget[testTarget.columns[1:]]\n",
    "    return sorted(zip(((testPred-testTarget)**2).mean(),testPred.columns), reverse=True)\n",
    "\n",
    "# checkPredictionsDotCsv(imDirPath/'dock'/'output'/'predictions.csv')\n",
    "\n",
    "# checkPredictionsDotCsv(imDirPath/'predictions3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run up to here to train orientation and joint location models.\n",
    "Then, run the four orientation and joint location training code sections below to train and save/export the four models (orientation and joint location for hands and feet).\n",
    "After doing this, you can train the RA label prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictedDupDFJoints = predictJoints(dupDF,'B')\n",
    "# predict joint locations on the training set for use in training label prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run up to here to train RA label prediction models.\n",
    "This can only be done after joint location models have been exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is run to train and export all the models for RA label prediction,\n",
    "# using all of the training data (no validation split).\n",
    "fullTrainAllNew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is run to update the Docker directory with the latest scripts and models\n",
    "updateDockerDir(str(imDirPath/'dock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - run through the prediction pipeline on part of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmallTestDF() : return pd.concat([mainDF[10:100],mainDF[200:320]]).reset_index(drop=True)\n",
    "smallTestDF = getSmallTestDF()\n",
    "smallTestDF[smallTestDF.columns[1:]] = 'notavail'\n",
    "smallTestDF.to_csv(imDirPath/'smalltest.csv',index=False)\n",
    "smallTestDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictAllNew(imDirPath/'smalltest.csv',imDirPath,imDirPath/'smalltest_out2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallTestPred = pd.read_csv(imDirPath/'smalltest_out2.csv')\n",
    "smallTestPred = smallTestPred[smallTestPred.columns[1:]] \n",
    "smallTestTarget = getSmallTestDF()\n",
    "smallTestTarget = smallTestTarget[smallTestTarget.columns[1:]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(((smallTestPred-smallTestTarget)**2).mean(),smallTestPred.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orientation and joint location training\n",
    "The following four sections are run once to train and save/export the models for orientation and joint location for hands and feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation training - hands\n",
    "This section is run once to train and save/export the hand orientation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmDF = makeFlipModDF(dupDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setupTraining(df=fmDF, **HOrientTrainPars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingHPars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiutils.randomSeedForTraining(22)\n",
    "learn.fit_one_cycle(3,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('HOrient_3epoch_0_01_wd_0_1')\n",
    "learn.export('HOrient_3epoch_0_01_wd_0_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation training - feet\n",
    "This section is run once to train and save/export the foot orientation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmDF = makeFlipModDF(dupDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setupTraining(df=fmDF, **FOrientTrainPars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingHPars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiutils.randomSeedForTraining(22)\n",
    "learn.fit_one_cycle(3,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('FOrient_3epoch_0_01_wd_0_1')\n",
    "learn.export('FOrient_3epoch_0_01_wd_0_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint location training using marked points - hands\n",
    "This section is run once to train and save/export the hand joint location model.\n",
    "This model predicts the location of 14 joints used in scoring\n",
    "(10 of these are finger joints used directly,\n",
    "while 4 are wrist points that are used for the 12 wrist joints used in scoring -\n",
    "to make the marking easier, I combined 3 wrist joints each into one marked point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setupTraining(**HJLocTrainPars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.show_batch(3,figsize=(9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tryCrossVal(40,0.04,**HJLocTrainPars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faiutils.randomSeedForTraining(22)\n",
    "learn.fit_one_cycle(40,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('HJoint_40epoch_0_04_wd_0_1')\n",
    "learn.export('HJoint_40epoch_0_04_wd_0_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint location training using marked points - feet\n",
    "This section is run once to train and save/export the foot joint location model.\n",
    "This model predicts the location of 6 toe joints used in scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "setupTraining(**FJLocTrainPars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.show_batch(3,figsize=(9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tryCrossVal(40,0.04,**FJLocTrainPars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faiutils.randomSeedForTraining(22)\n",
    "learn.fit_one_cycle(40,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('FJoint_40epoch_0_04_wd_0_1')\n",
    "learn.export('FJoint_40epoch_0_04_wd_0_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the joint location prediction:\n",
    "This section was used for a manual check on how good the predicted joint locations were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_validation = True  # check on validation only or on the whole dataset\n",
    "HOrF = 'H'  # check on hands or feet\n",
    "\n",
    "def setupJointHPars() :\n",
    "    if HOrF=='F' :\n",
    "        setupTraining(**FJLocTrainPars)\n",
    "    else :\n",
    "        setupTraining(**HJLocTrainPars)\n",
    "\n",
    "def dfToTensor(df,labelList) :\n",
    "    df = df[labelList]\n",
    "    return tensor(df.values)\n",
    "\n",
    "if check_validation :\n",
    "    # check predictions of one model after training, using the validation set\n",
    "    setupJointHPars()\n",
    "    modelName = HOrF+'Joint_40epoch_0_04_wd_0_1'\n",
    "    learn.load(modelName)\n",
    "    preds,target = learn.get_preds()\n",
    "    vpids = [os.path.basename(fp) for fp in data.valid_ds.items]\n",
    "else :\n",
    "    # check all hand or foot predictions on the whole dataset including training and validation sets,\n",
    "    # using previously trained exported models\n",
    "    labelList = addYX(allMarkedJointLabels(HOrF))\n",
    "    predictedJointsDF = predictJoints(dupDF,HOrF)\n",
    "    setupJointHPars()\n",
    "    target = dfToTensor(pd.read_csv(HOrF+'joints.csv'), labelList)\n",
    "    target = target*2.0 - 1.0  # rescale from 0..1 to the -1..1 range trained for by the NN\n",
    "    vpids = list(predictedJointsDF['Patient_ID'])\n",
    "    preds = dfToTensor(predictedJointsDF,labelList)\n",
    "\n",
    "print('MSE ', ((target.flatten()-preds.flatten())**2).mean().item())\n",
    "\n",
    "il = RAImageList.from_df(dupDF,imDirPath)\n",
    "il[0];  # to initialize il.jdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selectID = widgets.Select(options=vpids, rows=10)\n",
    "selectDisp = widgets.Select(options=['original marked',\n",
    "                                     'target (should be same as marked)',\n",
    "                                     'predicted'], rows=10)\n",
    "def updateIm() :\n",
    "    if selectDisp.value.startswith('target') :\n",
    "        pts = target[selectID.index]\n",
    "    elif selectDisp.value.startswith('predicted') :\n",
    "        pts = preds[selectID.index]\n",
    "    else :\n",
    "        pts = None\n",
    "    trainingHPars['debugMarkPts'] = pts\n",
    "    showPoints(il,selectID.value,pts)\n",
    "print('Select an image and target/predicted, then execute the next cell to see it.')\n",
    "display(widgets.AppLayout(left_sidebar=selectID, right_sidebar=selectDisp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "updateIm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging the joint location labelling:\n",
    "This section was used to debug feeding the marked joint locations into the Fastai training pipeline.\n",
    "\n",
    "### Part 1 - untransformed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingHPars = dict(defTrainingHPars, trHOrF='F',\n",
    "                     #trJoints=['F_mtp_XX__2','F_mtp_XX__ip'],\n",
    "                     trJoints=allMarkedJointLabels('F'),\n",
    "                     resizeToSize=raGuiDispImSize)\n",
    "il = RAImageList.from_df(dupDF,imDirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = il[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "showPoints(il,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getJointPoints(il,'UAB722R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "showPoints(il,'UAB722R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - transformed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False,flip_vert=False,\n",
    "                         max_rotate=trainingHPars['max_rotate'],\n",
    "                         max_lighting=trainingHPars['max_lighting'],\n",
    "                         max_warp=trainingHPars['max_warp'],\n",
    "                         max_zoom=trainingHPars['max_zoom'],\n",
    "                         )\n",
    "data = (il\n",
    "         .split_by_rand_pct(0.2,42)\n",
    "         .label_from_func(partial(getJointPoints,il),label_cls=PointsLabelList)\n",
    "         .transform(tfms, tfm_y=True, size=256)\n",
    "         .databunch().normalize(imagenet_stats)\n",
    "       )\n",
    "\n",
    "# code from the Biwi head pose example:\n",
    "# data = (PointsItemList.from_folder(path)\n",
    "#         .split_by_valid_func(lambda o: o.parent.name=='13')\n",
    "#         .label_from_func(get_ctr)\n",
    "#         .transform(get_transforms(), tfm_y=True, size=(120,160))\n",
    "#         .databunch().normalize(imagenet_stats)\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.show_batch(3,figsize=(9,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of performance testing/experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing hand joint narrowing prediction (all pip finger joints)\n",
    "trainUsingPredictedJoints(tryCrossVal,**dict(pointHPars['handTopJ'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing hand joint narrowing prediction (all mcp finger joints)\n",
    "trainUsingPredictedJoints(tryCrossVal,**dict(pointHPars['handMidJ'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing wrist narrowing prediction (cmc3, cmc4, cmc5)\n",
    "trainUsingCutJoint(tryCrossVal,**dict(cutHPars['wrist1'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing wrist narrowing prediction (mna, radcar, capnlun)\n",
    "trainUsingCutJoint(tryCrossVal,**dict(cutHPars['wrist2'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing foot joint narrowing prediction (all joints)\n",
    "trainUsingPredictedJoints(tryCrossVal,**dict(pointHPars['footJ'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing hand erosion prediction (all finger joints)\n",
    "trainUsingPredictedJoints(tryCrossVal,**dict(pointHPars['handTopAndMidE'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing wrist erosion prediction (nav, mul, mc1)\n",
    "trainUsingCutJoint(tryCrossVal,**dict(cutHPars['wrist3'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing wrist erosion prediction (ulna, lunate, radius)\n",
    "trainUsingCutJoint(tryCrossVal,**dict(cutHPars['wrist4'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing foot erosion prediction (all joints)\n",
    "trainUsingPredictedJoints(tryCrossVal,**dict(pointHPars['footE'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting on only one cross-validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing wrist narrowing prediction (cmc3, cmc4, cmc5)\n",
    "trainUsingCutJoint(tryTraining,**dict(cutHPars['wrist1'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing hand erosion prediction (all finger joints)\n",
    "trainUsingPredictedJoints(tryTraining,**dict(pointHPars['handTopAndMidE'],\n",
    "                                              # can try different hyperparameter values here: gBlur=3, etc.\n",
    "                                             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
